{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 - Let's get visual!.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZma_SZgUn4",
        "colab_type": "text"
      },
      "source": [
        "# 4. Let's get visual\n",
        "\n",
        "In the previous parts we have built up our intuition (and hopefully also understanding) of how we can automatically learn to see patterns in our data and use them to predict behaviour on hiterto unseen data.\n",
        "\n",
        "We have learnt the concept of Artificial Neural Networks.\n",
        "\n",
        "As it turns out, this concept is very powerful!\n",
        "\n",
        "### Universal Approximation Theorem\n",
        "\n",
        "In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters.\n",
        "\n",
        "Source: [Wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
        "\n",
        "In other words, ANNs are powerful tools for approximating a wide variety of functions.\n",
        "\n",
        "Let's use them to recognize hand-written digits.\n",
        "\n",
        "But first some initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKc9uI87gUn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "c06504c3-a97c-4c0c-a272-7d053b91303b"
      },
      "source": [
        "# Initialization stuff\n",
        "!gdown https://drive.google.com/uc?id=1LJccUzmjbD_tJ5TF3J3CkkeDNeItElLd\n",
        "!unzip cgML.zip\n",
        "\n",
        "%matplotlib inline\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable \n",
        "from scripts.helper import *\n",
        "plt.style.use('ggplot')\n",
        "# For reproducibility\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# if it does not exist, download mnist dataset\n",
        "root = \"datasets\"\n",
        "train_set = datasets.MNIST(root=root, train=True, transform=None, download=True)\n",
        "# We keep a copy of the dataset as is for visualization purposes\n",
        "train_set_viz = datasets.MNIST(root=root, train=True, transform=None, download=True)\n",
        "test_set = datasets.MNIST(root=root, train=False, transform=None, download=True)\n",
        "# We keep a copy of the dataset as is for visualization purposes\n",
        "test_set_viz = datasets.MNIST(root=root, train=False, transform=None, download=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LJccUzmjbD_tJ5TF3J3CkkeDNeItElLd\n",
            "To: /content/cgML.zip\n",
            "35.4MB [00:00, 112MB/s] \n",
            "Archive:  cgML.zip\n",
            "replace scripts/helper.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: scripts/helper.py       \n",
            "  inflating: scripts/__pycache__/helper.cpython-37.pyc  \n",
            "  inflating: .ipynb_checkpoints/1 - Facial Landmarks-checkpoint.ipynb  \n",
            "  inflating: .ipynb_checkpoints/1- Introduction - What is Machine Learning anyway_-checkpoint.ipynb  \n",
            "  inflating: .ipynb_checkpoints/2 - Automatic Learning-checkpoint.ipynb  \n",
            "  inflating: .ipynb_checkpoints/3 - A little inspiration from our brain-checkpoint.ipynb  \n",
            "  inflating: .ipynb_checkpoints/4 - Let's get visual!-checkpoint.ipynb  \n",
            "  inflating: .ipynb_checkpoints/5 - It ain't convoluted!-checkpoint.ipynb  \n",
            "  inflating: .ipynb_checkpoints/6 - Putting it all together-checkpoint.ipynb  \n",
            "  inflating: datasets/MNIST/processed/test.pt  \n",
            "  inflating: datasets/MNIST/processed/training.pt  \n",
            "  inflating: datasets/MNIST/raw/t10k-images-idx3-ubyte  \n",
            "  inflating: datasets/MNIST/raw/t10k-labels-idx1-ubyte  \n",
            "  inflating: datasets/MNIST/raw/train-images-idx3-ubyte  \n",
            "  inflating: datasets/MNIST/raw/train-labels-idx1-ubyte  \n",
            "  inflating: images/artificial-neural-network.png  \n",
            "  inflating: images/artificial-neuron.png  \n",
            "  inflating: images/birds.jpg        \n",
            "  inflating: images/cnn.png          \n",
            "  inflating: images/convolution.gif  \n",
            "  inflating: images/convolve-3d.png  \n",
            "  inflating: images/deconvnet.png    \n",
            "  inflating: images/gradient.png     \n",
            "  inflating: images/lady.png         \n",
            "  inflating: images/landscape.jpeg   \n",
            "  inflating: images/landscape2.jpg   \n",
            "  inflating: images/lion.png         \n",
            "  inflating: images/max-pooling-volume.png  \n",
            "  inflating: images/max-pooling.png  \n",
            "  inflating: images/neuron.png       \n",
            "  inflating: images/rgb-image.png    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX0yjeITgUn-",
        "colab_type": "text"
      },
      "source": [
        "### The MNIST dataset\n",
        "\n",
        "The MNIST dataset probably constitutes the **_Hello world_** of Machine Learning.\n",
        "\n",
        "It is a set of images of hand-written digits.\n",
        "\n",
        "This is not a toy dataset. Let's see how many images it contains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFYeh25OgUoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e5a4a021-5a93-402d-8908-5c28d393ec32"
      },
      "source": [
        "print(\"MNIST training set data contains {} images\".format(len(train_set)))\n",
        "print(\"MNIST test set data contains {} images\".format(len(test_set)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST training set data contains 60000 images\n",
            "MNIST test set data contains 10000 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpklM-4cgUoG",
        "colab_type": "text"
      },
      "source": [
        "So far we have been dealing with training data that fits in the memory.\n",
        "\n",
        "Each iteration in our training loop performed **_gradient descent_** on the **_cost function_** that was defined over the entire dataset.\n",
        "\n",
        "With so much more data, that would be very slow.\n",
        "\n",
        "Thus we need to performing training iterations on a subset of the data.\n",
        "\n",
        "We call this subset a **_batch_** of data. We decide on a **_batch size_** and the cost is computed over each batch in our training loop.\n",
        "\n",
        "As opposed to **_gradient descent_** performed over the entire training data, this **_batch_** based strategy is called **_stochastic gradient descent_**.\n",
        "\n",
        "The iteration over all the batches in the training data is called the completion of an **_epoch_**.\n",
        "\n",
        "While the size of each batch is fixed, it is common practice to shuffle the data at the completion of an **_epoch_**. This results in differnt data points in each batch every epoch i.e batch 0 in epoch 0 will be different from batch 0 in epoch 1.\n",
        "\n",
        "We will use all the concepts we just covered, but first let's examine some images from the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4vpxhTJgUoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "67ef74ed-8879-4198-abbb-bd05bac25820"
      },
      "source": [
        "# Let's look at 9 images\n",
        "im_list = [train_set[i][0] for i in range(9)]\n",
        "thumb_grid(im_list, (3, 3))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 18 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvWnKUkFgUoN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "efcc19c4-5715-4e9c-ccf9-c64e462ff408"
      },
      "source": [
        "# What's the size of each image?\n",
        "im_list[0].size"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omDyGX2rgUoS",
        "colab_type": "text"
      },
      "source": [
        "#### Images as data\n",
        "\n",
        "How do we treat these images as data?\n",
        "\n",
        "Each image is 28x28 pixels. Thus we could treat each image as an input of size 28x28."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et3-_tFxgUoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1101
        },
        "outputId": "6c0c3843-92a5-441c-d2d1-9845d5f2882c"
      },
      "source": [
        "# Treat image as data\n",
        "np.array(im_list[0]).reshape((1, 28*28))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "        126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "        253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "        253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "        253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "        253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "        183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "        229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "        221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "        213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "        219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "        226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrRpeRp-gUoZ",
        "colab_type": "text"
      },
      "source": [
        "Fantastic! We can now use our knowledge from the previous sections to create an ANN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Be0S0XgUob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input consist of size 28x28 \n",
        "# First hidden layer of 500 units\n",
        "# Second hidden layer 256 units\n",
        "# Finally the output is 10 units each unit representing a digit\n",
        "n_in, n_h1, n_h2, n_out = 28*28, 500, 256, 10\n",
        "\n",
        "# This will flatten the input\n",
        "class Flatten(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        return x\n",
        "\n",
        "# Here we will use the Sequential model api for a cleaner implementation\n",
        "model = torch.nn.Sequential(Flatten(),\n",
        "                            torch.nn.Linear(n_in, n_h1),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(n_h1, n_h2),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(n_h2, n_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu80IZUugUog",
        "colab_type": "text"
      },
      "source": [
        "This is a pretty big model! Let's see how many weights we will be learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e-SpmXbgUoh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5982341f-cf62-45d3-9cb4-ea8149a3f8da"
      },
      "source": [
        "print(\"Number of parameters (weights + bias terms) we will learn: {}\".format(trainable_parameters(model)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters (weights + bias terms) we will learn: 523326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fjsoT4AgUon",
        "colab_type": "text"
      },
      "source": [
        "We will be learning the values of more than half a million parameters!\n",
        "\n",
        "Pytorch provides us with a convenient method of getting batches of data through **_DataLoaders_**.\n",
        "\n",
        "Let's define our data loaders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn8UxMFNgUoo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3c5ae8b2-4377-4d8a-a74c-9b8e0e50ef53"
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "# We want to transform each image to a tensor and perform some normalization\n",
        "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "train_set.transform = trans\n",
        "test_set.transform = trans\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)\n",
        "\n",
        "print(\"The number of batches in one epoch over the training set is {}\".format(len(train_loader)))\n",
        "print(\"The number of batches in one epoch over the test set is {}\".format(len(test_loader)))\n",
        "\n",
        "# Let's see the help for the Normalize function\n",
        "?transforms.Normalize"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of batches in one epoch over the training set is 600\n",
            "The number of batches in one epoch over the test set is 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGUQugMQgUov",
        "colab_type": "text"
      },
      "source": [
        "We can see that the numbers shown above make sense. After all, we had 60,000 images in the training set. We have set our **_batch size_** to 100. Therefore, we should have 600 batches per **_epoch_**.\n",
        "\n",
        "We added some transforms to our datasets. Let's look at what we get as input data now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWNyrvBugUox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2008
        },
        "outputId": "6664df9f-b41f-4286-8da9-84884223e336"
      },
      "source": [
        "train_set[0][0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4882, -0.4294,\n",
              "          -0.4294, -0.4294, -0.0059,  0.0333,  0.1863, -0.3980,  0.1510,\n",
              "           0.5000,  0.4686, -0.0020, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.3824, -0.3588, -0.1314,  0.1039,  0.1667,  0.4922,\n",
              "           0.4922,  0.4922,  0.4922,  0.4922,  0.3824,  0.1745,  0.4922,\n",
              "           0.4490,  0.2647, -0.2490, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.3078,  0.4333,  0.4922,  0.4922,  0.4922,  0.4922,  0.4922,\n",
              "           0.4922,  0.4922,  0.4922,  0.4843, -0.1353, -0.1784, -0.1784,\n",
              "          -0.2804, -0.3471, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.4294,  0.3588,  0.4922,  0.4922,  0.4922,  0.4922,  0.4922,\n",
              "           0.2765,  0.2137,  0.4686,  0.4451, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.1863,  0.1118, -0.0804,  0.4922,  0.4922,  0.3039,\n",
              "          -0.4569, -0.5000, -0.3314,  0.1039, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.4451, -0.4961,  0.1039,  0.4922, -0.1471,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000,  0.0451,  0.4922,  0.2451,\n",
              "          -0.4922, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.4569,  0.2451,  0.4922,\n",
              "          -0.2255, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.3627,  0.4451,\n",
              "           0.3824,  0.1275, -0.0765, -0.4961, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.1824,\n",
              "           0.4412,  0.4922,  0.4922, -0.0333, -0.4020, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.3235,  0.2294,  0.4922,  0.4922,  0.0882, -0.3941, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.4373, -0.1353,  0.4882,  0.4922,  0.2333, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000,  0.4765,  0.4922,  0.4765, -0.2490,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.3196,  0.0098,  0.2176,  0.4922,  0.4922,  0.3118, -0.4922,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.3471,  0.0804,\n",
              "           0.3980,  0.4922,  0.4922,  0.4922,  0.4804,  0.2137, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.4059, -0.0529,  0.3667,  0.4922,\n",
              "           0.4922,  0.4922,  0.4922,  0.2882, -0.1941, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.4098, -0.2412,  0.3353,  0.4922,  0.4922,  0.4922,\n",
              "           0.4922,  0.2765, -0.1824, -0.4922, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4294,\n",
              "           0.1706,  0.3588,  0.4922,  0.4922,  0.4922,  0.4922,  0.2647,\n",
              "          -0.1863, -0.4647, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.2843,  0.1745,  0.3863,\n",
              "           0.4922,  0.4922,  0.4922,  0.4922,  0.4569,  0.0216, -0.4569,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000,  0.0333,  0.4922,  0.4922,\n",
              "           0.4922,  0.3314,  0.0294,  0.0176, -0.4373, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
              "         [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
              "          -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc664LIngUo4",
        "colab_type": "text"
      },
      "source": [
        "We are effectively normalizing the data to have a mean of 0.5 and a standard deviation of 1. Another common technique that is often used is subtracting 127.5 from each pixel value and then dividing by 255.\n",
        "\n",
        "Let's look at the shape of the item"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM6poG7EgUo5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e7998e8-3d30-4d8a-ecba-2993adaf9fd9"
      },
      "source": [
        "train_set[0][0].shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrPmJ-AgUpA",
        "colab_type": "text"
      },
      "source": [
        "### Cross entropy loss\n",
        "\n",
        "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
        "\n",
        "Since our output is 10 nodes each giving the probability of a digit, this measure for our loss function makes sense.\n",
        "\n",
        "We'll use it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGSc1J3hgUpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1084
        },
        "outputId": "32e14430-9ef3-4252-d492-1c63c2e430ba"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    # trainning\n",
        "    ave_loss = 0\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        #if use_cuda:\n",
        "        #    x, target = x.cuda(), target.cuda()\n",
        "        x, target = Variable(x), Variable(target)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, target)\n",
        "        # Use an exponentially weighted average for running loss value\n",
        "        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
        "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
        "                epoch, batch_idx+1, ave_loss))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==>>> epoch: 0, batch index: 100, train loss: 0.939570\n",
            "==>>> epoch: 0, batch index: 200, train loss: 0.457600\n",
            "==>>> epoch: 0, batch index: 300, train loss: 0.372079\n",
            "==>>> epoch: 0, batch index: 400, train loss: 0.398162\n",
            "==>>> epoch: 0, batch index: 500, train loss: 0.322596\n",
            "==>>> epoch: 0, batch index: 600, train loss: 0.309751\n",
            "==>>> epoch: 1, batch index: 100, train loss: 0.284283\n",
            "==>>> epoch: 1, batch index: 200, train loss: 0.260029\n",
            "==>>> epoch: 1, batch index: 300, train loss: 0.273299\n",
            "==>>> epoch: 1, batch index: 400, train loss: 0.241561\n",
            "==>>> epoch: 1, batch index: 500, train loss: 0.230588\n",
            "==>>> epoch: 1, batch index: 600, train loss: 0.220963\n",
            "==>>> epoch: 2, batch index: 100, train loss: 0.216370\n",
            "==>>> epoch: 2, batch index: 200, train loss: 0.168218\n",
            "==>>> epoch: 2, batch index: 300, train loss: 0.181308\n",
            "==>>> epoch: 2, batch index: 400, train loss: 0.155947\n",
            "==>>> epoch: 2, batch index: 500, train loss: 0.155363\n",
            "==>>> epoch: 2, batch index: 600, train loss: 0.138144\n",
            "==>>> epoch: 3, batch index: 100, train loss: 0.141204\n",
            "==>>> epoch: 3, batch index: 200, train loss: 0.163818\n",
            "==>>> epoch: 3, batch index: 300, train loss: 0.146791\n",
            "==>>> epoch: 3, batch index: 400, train loss: 0.156303\n",
            "==>>> epoch: 3, batch index: 500, train loss: 0.143215\n",
            "==>>> epoch: 3, batch index: 600, train loss: 0.130086\n",
            "==>>> epoch: 4, batch index: 100, train loss: 0.106120\n",
            "==>>> epoch: 4, batch index: 200, train loss: 0.126328\n",
            "==>>> epoch: 4, batch index: 300, train loss: 0.107290\n",
            "==>>> epoch: 4, batch index: 400, train loss: 0.115978\n",
            "==>>> epoch: 4, batch index: 500, train loss: 0.104256\n",
            "==>>> epoch: 4, batch index: 600, train loss: 0.112974\n",
            "==>>> epoch: 5, batch index: 100, train loss: 0.094641\n",
            "==>>> epoch: 5, batch index: 200, train loss: 0.086382\n",
            "==>>> epoch: 5, batch index: 300, train loss: 0.100540\n",
            "==>>> epoch: 5, batch index: 400, train loss: 0.085990\n",
            "==>>> epoch: 5, batch index: 500, train loss: 0.093479\n",
            "==>>> epoch: 5, batch index: 600, train loss: 0.093540\n",
            "==>>> epoch: 6, batch index: 100, train loss: 0.070682\n",
            "==>>> epoch: 6, batch index: 200, train loss: 0.076685\n",
            "==>>> epoch: 6, batch index: 300, train loss: 0.071574\n",
            "==>>> epoch: 6, batch index: 400, train loss: 0.087620\n",
            "==>>> epoch: 6, batch index: 500, train loss: 0.064167\n",
            "==>>> epoch: 6, batch index: 600, train loss: 0.088859\n",
            "==>>> epoch: 7, batch index: 100, train loss: 0.061660\n",
            "==>>> epoch: 7, batch index: 200, train loss: 0.073964\n",
            "==>>> epoch: 7, batch index: 300, train loss: 0.055158\n",
            "==>>> epoch: 7, batch index: 400, train loss: 0.057169\n",
            "==>>> epoch: 7, batch index: 500, train loss: 0.070929\n",
            "==>>> epoch: 7, batch index: 600, train loss: 0.081923\n",
            "==>>> epoch: 8, batch index: 100, train loss: 0.055247\n",
            "==>>> epoch: 8, batch index: 200, train loss: 0.053798\n",
            "==>>> epoch: 8, batch index: 300, train loss: 0.074237\n",
            "==>>> epoch: 8, batch index: 400, train loss: 0.058638\n",
            "==>>> epoch: 8, batch index: 500, train loss: 0.062958\n",
            "==>>> epoch: 8, batch index: 600, train loss: 0.064939\n",
            "==>>> epoch: 9, batch index: 100, train loss: 0.049918\n",
            "==>>> epoch: 9, batch index: 200, train loss: 0.062623\n",
            "==>>> epoch: 9, batch index: 300, train loss: 0.062061\n",
            "==>>> epoch: 9, batch index: 400, train loss: 0.056283\n",
            "==>>> epoch: 9, batch index: 500, train loss: 0.052107\n",
            "==>>> epoch: 9, batch index: 600, train loss: 0.062353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHylep5vgUpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "c8718d71-a955-40f7-bac5-544ec195b3c0"
      },
      "source": [
        "# Let's try our model on the first 9 rows of our training data\n",
        "im_list = [train_set_viz[i][0] for i in range(9)]\n",
        "thumb_grid(im_list, (3, 3))\n",
        "\n",
        "y_hat = []\n",
        "for i in range(9):\n",
        "    X = Variable(train_set[i][0])\n",
        "    y_hat.append(np.argmax(model(X).detach().numpy()))\n",
        "print(np.array(y_hat).reshape((3,3)))\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5 0 4]\n",
            " [1 9 2]\n",
            " [1 3 1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 18 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9FybcFxgUpP",
        "colab_type": "text"
      },
      "source": [
        "Great! We now know that our model has learn't something.\n",
        "\n",
        "Let us now evaluate the model's accuracy on the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXLgXcvwgUpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df08c1df-6dcd-4539-edd2-2973465a0959"
      },
      "source": [
        "num_correct = 0\n",
        "num_wrong = 0\n",
        "for i in range(len(test_set)):\n",
        "    X = Variable(test_set[i][0])\n",
        "    y_hat = np.argmax(model(X).detach().numpy())\n",
        "    if y_hat == test_set[i][1]:\n",
        "        num_correct += 1\n",
        "    else:\n",
        "        num_wrong += 1\n",
        "accuracy = num_correct / (num_correct + num_wrong)\n",
        "print(\"Model accuracy on test data is {}\".format(accuracy))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model accuracy on test data is 0.9764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCmQgzhjgUpU",
        "colab_type": "text"
      },
      "source": [
        "Fantastic! We have trained a model that can recognize hand-written digits with 97.49% accuracy.\n",
        "\n",
        "## What just happened?\n",
        "\n",
        "1. We started off by looking at images as data.\n",
        "2. We examined the MNIST dataset and saw that it's a large dataset with 60,000 training images\n",
        "3. We understood the concepts of **_batch size_**, **_epochs_**, and **_mini-batch_** based learning\n",
        "4. We understood how **_gradient descent_** can be modified to work with  **_mini-batches_** to form the **_stochastic gradient descent_** algorithm.\n",
        "5. We looked at the **_DataLoader_** class from Pytorch for supplying us with **_mini-batches_** while shuffling the data every **_epoch_**\n",
        "6. We used our knowledge from the previous sections to define an **_ANN_** (also called a **_Multi-layer Perceptron_** or **_MLP_**) with 500 and 256 neurons in the hidden layers.\n",
        "7. We observed that we are learning more that half a million parameters.\n",
        "8. We trained our **_MLP_** on the training data.\n",
        "9. We evaluated our model's accuracy on the test data.\n",
        "10. We observed that we could achieve a 97.49% accuracy on unseen test data in just 10 training epochs.\n",
        "\n",
        "## Can we do better?\n",
        "\n",
        "Treating image pixels as input \"**_features_**\" results in a lot of weights! Can we make our models smaller?\n",
        "\n",
        "In the next section, we will look at a new kind of network called a **_Convolutional Neural Network_**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ac0nLugUpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}